{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2c09de",
   "metadata": {},
   "source": [
    "## Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60f748fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns \n",
    "from pymongo import MongoClient\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c47ffb",
   "metadata": {},
   "source": [
    "## Initialisation du Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f6bce",
   "metadata": {},
   "source": [
    "The __get_token_from_file__ fonction\n",
    "   \n",
    "Description : Retrieve the token from the token.json file. This token has renewed instantly in the  /admindata.atmo-france.org/api/doc website by clik to a button.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the token.json file.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The token value.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fc5e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_from_file(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        return data.get('token', None)\n",
    "    \n",
    "api_key = get_token_from_file('token.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16dac03",
   "metadata": {},
   "source": [
    "## Fonctions Secondaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387f1bf",
   "metadata": {},
   "source": [
    "   The __fetch_atmo_data__ fonction\n",
    "    \n",
    "    \n",
    "   Description : Fetches data from AtmoData API.\n",
    "    \n",
    "    Args:\n",
    "    - api_key (str): API key for authentication.\n",
    "    - id_data (int): Identifier of the data.\n",
    "    - params (dict): Parameters for the query.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Tuple containing the DataFrame of JSON response from the API and the response status code.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2ed58",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ef57299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_atmo_data(api_key, id_data, params):\n",
    "\n",
    "    # Construction de l'URL avec l'identifiant de la donnée\n",
    "    url = f\"https://admindata.atmo-france.org/api/data/{id_data}/\"\n",
    "    \n",
    "    # Convertir les paramètres en une chaîne JSON et les ajouter à l'URL\n",
    "    url += json.dumps(params)\n",
    "    \n",
    "    url += \"?withGeom=false\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "       \n",
    "    if response.status_code == 200:\n",
    "        data = json.loads(response.content)\n",
    "        return pd.json_normalize(data['features']), response.status_code\n",
    "    elif response.status_code == 401 or response.status_code == 400: \n",
    "        #sys.exit(f\"Erreur: {response.status_code}\") \n",
    "        return None, response.status_code\n",
    "    else:\n",
    "        return None, response.status_code\n",
    "        #sys.exit(f\"Erreur: {response.status_code}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3f5ba",
   "metadata": {},
   "source": [
    "   The __add_data_to_database()__ function\n",
    "    \"\"\"\n",
    "    Add data to MongoDB collection based on id_data.\n",
    "    \n",
    "    Args:\n",
    "    - collection (pymongo.collection.Collection): MongoDB collection.\n",
    "    - data_dict (dict): Dictionary containing data to be added to the collection.\n",
    "    - id_data (int): Identifier of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82fe64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_to_database(data_dict, id_data):\n",
    "\n",
    "    if id_data == 112:\n",
    "        for entry in data_dict:\n",
    "            aire_quality.insert_one(entry)\n",
    "    elif id_data == 113 :\n",
    "        for entry in data_dict:\n",
    "            pollution_113.insert_one(entry)\n",
    "            \n",
    "    elif  id_data == 114:\n",
    "        for entry in data_dict:\n",
    "            pollution_114.insert_one(entry)\n",
    "            \n",
    "    elif  id_data == 119:\n",
    "        for entry in data_dict:\n",
    "            emission_region.insert_one(entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9056441",
   "metadata": {},
   "source": [
    "\n",
    "The __write_trace_data_file__() function\n",
    "    \n",
    "   Description : Writes information about zone (commune/region) without data to a trace file.\n",
    "    \n",
    "    Args:\n",
    "    - code_commune (int): Code of the commune.\n",
    "    - id_data (int): Identifier of the data.\n",
    "    - start_date (str): Start date in the format \"YYYY-MM-DD\".\n",
    "    - stop_date (str): Stop date in the format \"YYYY-MM-DD\".\n",
    "    - trace_file (str): Path to the CSV file to store the trace of communes without results.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efcbe7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_trace_data_to_file(code_zone, id_data, start_date, trace_file):\n",
    "\n",
    "    # Create DataFrame with trace data\n",
    "    trace_data = {\n",
    "        \"code_zone\": code_zone,\n",
    "        \"id_data\": id_data,\n",
    "        \"start_date\": start_date\n",
    "    }\n",
    "    trace_df = pd.DataFrame([trace_data])\n",
    "    \n",
    "    # Write trace data to CSV file\n",
    "    trace_df.to_csv(trace_file, mode=\"a\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1acd61",
   "metadata": {},
   "source": [
    "### A compléter pour les autres types de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142826c",
   "metadata": {},
   "source": [
    " The __clean_data()__ function  \n",
    "   \n",
    "   Description : Clean commune data DataFrame by removing \"geometry\" column and renaming columns starting with \"properties.\".\n",
    "    \n",
    "    Args:\n",
    "    - commune_data (pd.DataFrame): DataFrame containing commune data.\n",
    "    - id_data (int): Identifier of the data.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Cleaned DataFrame.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40f76503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(commune_data, id_data):\n",
    "    \n",
    "    # Rename columns starting with \"properties.\"\n",
    "    commune_data.rename(columns=lambda x: x.replace(\"properties.\", \"\"), inplace=True)\n",
    "    if id_data == 112:\n",
    "        commune_data.drop(columns=[\"geometry\"], inplace=True, errors=\"ignore\")\n",
    "        commune_data.drop(columns=[\"type\"], inplace=True, errors=\"ignore\")\n",
    "        commune_data.drop(columns=[\"x_wgs84\"], inplace=True, errors=\"ignore\")\n",
    "        commune_data.drop(columns=[\"x_reg\"], inplace=True, errors=\"ignore\")\n",
    "        commune_data.drop(columns=[\"y_reg\"], inplace=True, errors=\"ignore\")\n",
    "        commune_data.drop(columns=[\"y_wgs84\"], inplace=True, errors=\"ignore\")\n",
    "        commune_data.drop(columns=[\"type_zone\"], inplace=True, errors=\"ignore\")\n",
    "        commune_data.drop(columns=[\"y_reg\"], inplace=True, errors=\"ignore\")\n",
    "        commune_data.drop(columns=[\"epsg_reg\"], inplace=True, errors=\"ignore\")\n",
    "        \n",
    "    elif ((id_data == 113) or (id_data == 114)):\n",
    "        # Remove the \"type\" column\n",
    "        commune_data.drop(columns=[\"geometry\"], inplace=True, errors=\"ignore\")\n",
    "        commune_data.drop(columns=[\"type\"], inplace=True, errors=\"ignore\")\n",
    "    else :\n",
    "        commune_data.drop(columns=[\"geometry\"], inplace=True, errors=\"ignore\")\n",
    "        # A compléter attend de voir la structures des résultats avec une id_data == 119\n",
    "        \n",
    "    return commune_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a179424",
   "metadata": {},
   "source": [
    "The __update_status()__ function\n",
    "\n",
    "    Description :    Updates the status of the processed commune in the Excel file by marking it as processed for the specified id_data.\n",
    "    \n",
    "    Args:\n",
    "    - code_insee_file (str): Path to the Excel file containing commune codes INSEE and department codes.\n",
    "    - zone_code (int): Code of the zone to update.\n",
    "    - colonne_verification (str): Column verification value (\"verif_112\", \"verif_113\", \"verif_114\", \"verif_119\").\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30624a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(code_insee_file, zone_code, colonne_verification):\n",
    "\n",
    "    # Mapping des valeurs de colonne_verification aux indices de colonne correspondants\n",
    "    colonne_indices = {\n",
    "        \"verif_112\": 6,\n",
    "        \"verif_113\": 7,\n",
    "        \"verif_114\": 8,\n",
    "        \"verif_119\": 9\n",
    "    }\n",
    "    \n",
    "    # Vérifier si colonne_verification est une valeur valide\n",
    "    if colonne_verification not in colonne_indices:\n",
    "        print(\"Erreur: colonne_verification invalide.\")\n",
    "        return\n",
    "    \n",
    "    # Load the Excel file as a template\n",
    "    wb = load_workbook(code_insee_file)\n",
    "    \n",
    "    # Select the active worksheet\n",
    "    ws = wb.active\n",
    "    \n",
    "    # Find the row index corresponding to the commune code\n",
    "    for idx, row in enumerate(ws.iter_rows(min_row=2, max_col=ws.max_column, max_row=ws.max_row), start=2):\n",
    "        if row[1].value == zone_code:  \n",
    "            # Update the status of the commune for the specified id_data (assuming id_data is the column name)\n",
    "            ws.cell(row=idx, column=colonne_indices[colonne_verification]).value = 1\n",
    "            break\n",
    "    \n",
    "    # Save the modified Excel file\n",
    "    wb.save(code_insee_file)\n",
    "    wb.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7be04b",
   "metadata": {},
   "source": [
    "## Fonctions principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958d6b9",
   "metadata": {},
   "source": [
    "The __store_commune()__ function\n",
    "\n",
    "Description : Store data for a specific commune from the AtmoData API and updates its status.\n",
    "    \n",
    "    Args:\n",
    "    - api_key (str): API key for authentication.\n",
    "    - id_data (int): Identifier of the data (112 for air quality indices).\n",
    "    - start_date (str): Start date in the format \"YYYY-MM-DD\".\n",
    "    - commune_code (int): Code of the commune to fetch data for.\n",
    "    - code_insee_file (str): Path to the Excel file containing commune codes INSEE and department codes.\n",
    "    - trace_file (str): Path to the CSV file to store the trace of communes without results.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ceed4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_commune(api_key, id_data, start_date, commune_code, code_insee_file, trace_file):\n",
    "    colonne_verification = \"verif_\" + str(id_data)\n",
    "    \n",
    "    # Define the parameters for the query\n",
    "    params = {\n",
    "        \"code_zone\": {\"operator\": \"=\", \"value\": commune_code},\n",
    "        \"date_ech\": {\"operator\": \">=\", \"value\": start_date}\n",
    "    }\n",
    "    \n",
    "    # Fetch data for the current commune\n",
    "    commune_data, status_api = fetch_atmo_data(api_key, id_data, params)\n",
    "    \n",
    "    # Append the data to the DataFrame\n",
    "    if commune_data is not None and not commune_data.empty:\n",
    "        commune_data = clean_data(commune_data, id_data)\n",
    "        \n",
    "        # Add to database\n",
    "        add_data_to_database(commune_data.to_dict(orient='records'), id_data)\n",
    "    else:\n",
    "        if status_api == 400 or status_api == 401: \n",
    "            sys.exit(\"Renouveler le token\")\n",
    "        else: \n",
    "            # Write trace data to file for commune without data\n",
    "            write_trace_data_to_file(commune_code, id_data, start_date, trace_file)\n",
    "\n",
    "    # Update the status of the processed commune\n",
    "    update_status(code_insee_file, commune_code, colonne_verification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1723331e",
   "metadata": {},
   "source": [
    " The __store departement()__ function\n",
    " \n",
    " Description : store data for all communes in a specific department from the AtmoData API.\n",
    "    \n",
    "    Args:\n",
    "    - api_key (str): API key for authentication.\n",
    "    - id_data (int): Identifier of the data (112 for air quality indices).\n",
    "    - start_date (str): Start date in the format \"YYYY-MM-DD\".\n",
    "    - code_insee_file (str): Path to the Excel file containing commune codes INSEE and department codes.\n",
    "    - code_dept (int): Code of the department.\n",
    "    - trace_file (str): Path to the CSV file to store the trace of communes without results.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df28852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_department(api_key, id_data, start_date, code_insee_file, code_dept, trace_file):\n",
    "\n",
    "    colonne_verification = \"verif_\"+str(id_data)\n",
    "    # Read commune codes INSEE and department codes from the Excel file\n",
    "    df_communes = pd.read_excel(code_insee_file)\n",
    "    # Filter communes for the specified department and those not processed for id_data\n",
    "    dept_communes = df_communes[(df_communes[colonne_verification] == 0) & (df_communes[\"DEP\"] == code_dept)]\n",
    "    # Iterate over all communes in the specified department\n",
    "    for _, commune in dept_communes.iterrows():\n",
    "        commune_code = commune[\"COM\"]\n",
    "        \n",
    "        # store data and update status for the current commune\n",
    "        store_commune(api_key, id_data, start_date, commune_code, code_insee_file, trace_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49558c9d",
   "metadata": {},
   "source": [
    "The __store_region__ function  \n",
    "\"\"\"\n",
    "    Store data for all communes in all departments of a specific region from the AtmoData API.\n",
    "    \n",
    "    Args:\n",
    "    - api_key (str): API key for authentication.\n",
    "    - id_data (int): Identifier of the data (112 for air quality indices).\n",
    "    - start_date (str): Start date in the format \"YYYY-MM-DD\".\n",
    "    - region_code (int): Code of the region.\n",
    "    - code_insee_file (str): Path to the Excel file containing commune codes INSEE and department codes.\n",
    "    - trace_file (str): Path to the CSV file to store the trace of communes without results.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee4ccda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_region(api_key, id_data, start_date, region_code, code_insee_file, trace_file):\n",
    "\n",
    "    # Get the list of departments in the region from the Excel file\n",
    "    df = pd.read_excel(code_insee_file)\n",
    "    region_departments = df[df[\"REG\"] == region_code][\"DEP\"].unique()\n",
    "    \n",
    "    # Iterate over all departments in the region\n",
    "    for dept_code in region_departments:\n",
    "        #\n",
    "        api_key = get_token_from_file('token.json')\n",
    "        #\n",
    "        # Store data for the current department\n",
    "        store_department(api_key, id_data, start_date, code_insee_file, dept_code, trace_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431d439",
   "metadata": {},
   "source": [
    "The __fetch_and_store_region()__ function :\n",
    "   \n",
    "Description : Fetches and store data for a specific region from the AtmoData API and updates its status.\n",
    "    \n",
    "    Args:\n",
    "    - api_key (str): API key for authentication.\n",
    "    - id_data (int): Identifier of the data (112 for air quality indices).\n",
    "    - start_date (str): Start date .\n",
    "    - region_code (int): Code of the region to fetch data for.\n",
    "    - code_insee_file (str): Path to the Excel file containing commune codes INSEE and department codes.\n",
    "    - trace_file (str): Path to the CSV file to store the trace of communes without results.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d6f1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_store_region(api_key, id_data, start_date, region_code, code_insee_file, trace_file):\n",
    "\n",
    "    colonne_verification = \"verif_\" + str(id_data)\n",
    "    \n",
    "    # Define the parameters for the query\n",
    "    params = {\n",
    "        \"code_zone\": {\"operator\": \"=\", \"value\": int(region_code)},\n",
    "        \"date_ech\": {\"operator\": \">=\", \"value\": start_date}\n",
    "    }\n",
    "    \n",
    "    # Fetch data for the current region\n",
    "    region_data, status_api = fetch_atmo_data(api_key, id_data, params)\n",
    "    \n",
    "    # Append the data to the DataFrame\n",
    "    if region_data is not None and not region_data.empty:\n",
    "        region_data = clean_data(region_data, id_data)\n",
    "        \n",
    "        # Add to database\n",
    "        add_data_to_database(region_data.to_dict(orient='records'), id_data)\n",
    "    else:\n",
    "        if status_api == 400 or status_api == 401: \n",
    "            sys.exit(\"Renouveler le token\")\n",
    "        else: \n",
    "            # Write trace data to file for region without data\n",
    "            write_trace_data_to_file(region_code, id_data, start_date, trace_file)\n",
    "\n",
    "    # Update the status of the processed region\n",
    "    update_status(code_insee_file, region_code, colonne_verification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb297d",
   "metadata": {},
   "source": [
    "The __general_store()__ function :\n",
    "\n",
    "Description : Store data for all communes in all regions or for all region if id_data==119, based on the provided data identifier.\n",
    "    \n",
    "    Args:\n",
    "    - api_key (str): API key for authentication.\n",
    "    - id_data (int): Identifier of the data (112, 113, 114, or 119 for air quality indices).\n",
    "    - start_date (str): Start date in the format \"YYYY-MM-DD\".\n",
    "    - code_insee_file (str): Path to the Excel file containing commune codes INSEE and department codes.\n",
    "    - trace_file (str): Path to the CSV file to store the trace of communes without results.\n",
    "    \"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2243b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_store(api_key, id_data, start_date, code_insee_file, trace_file):\n",
    "\n",
    "    colonne_verification = \"verif_\" + str(id_data)\n",
    "    # Read commune codes INSEE, department codes, and region codes from the Excel file\n",
    "    df_communes = pd.read_excel(code_insee_file)\n",
    "    \n",
    "    # Get unique region codes from the DataFrame\n",
    "    region_codes = df_communes[\"REG\"].unique()\n",
    "    # Iterate over all unique region codes\n",
    "    for region_code in region_codes:\n",
    "        # Fetch data for the region based on the data identifier\n",
    "        if id_data in [112, 113, 114]:\n",
    "            store_region(api_key, id_data, start_date, region_code, code_insee_file, trace_file)\n",
    "        elif id_data == 119:\n",
    "            fetch_and_store_region(api_key, id_data, start_date, region_code, code_insee_file, trace_file)\n",
    "        else:\n",
    "            # Error handling for invalid id_data\n",
    "            print(f\"Error: Unsupported id_data {id_data}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad37656",
   "metadata": {},
   "source": [
    "## Acquision des données selon divers paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457b371",
   "metadata": {},
   "source": [
    "Maintenant que nous avons défini nos différentes fonctions, nous allons ensuite définir les paramètres et appeler la fonction principale general_store pour stocker progressivement nos différents type de données (indice aire, pollution de l'année dernière, pollution constaté la veille, le jour même et prévu le lendemin ainsi que les émissions des régions), en ajustant juste le paramètre id_data( resp. 112, 113, 114 ou 119). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7c9e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion à la base de données MongoDB\n",
    "client = MongoClient('mongodb://localhost', 27017)\n",
    "db = client['climat_france'] \n",
    "aire_quality = db['aire_quality']\n",
    "pollution_113 = db['pollution_113']\n",
    "pollution_114 = db['pollution_114']\n",
    "emission_region = db['emission_region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fd88184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le type de données à recupérer (112: indice qualité de l'air) \n",
    "id_data = 112 \n",
    "# Date de début de la plage de recherche, la date de fin est au moment d'exécution de la requête\n",
    "start_date = \"01-01-2021\"\n",
    "# fichier contenant dans les codes insee, des communes, départements et régions.\n",
    "# Mais aussi des colonnes supplémentaires pour chaque id_data pour vérifier les \n",
    "# communes/régions déja balayés pour l'id_data spécifique.\n",
    "code_insee_file = \"commune_insee.xlsx\"\n",
    "# Contients les informations des communes qui n'ont pas donnés de résultats pour un id_data donné.\n",
    "# On les stock par précaution, mais n'ont présentement une utilité particulière.\n",
    "trace_file = \"empty_trace_com.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to fetch air quality data for the Paris region since the start of last year\n",
    "general_store(api_key, id_data, start_date, code_insee_file, trace_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72431529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
